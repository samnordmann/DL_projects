{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f8cc11f",
   "metadata": {},
   "source": [
    "# Implementation of the Transformer architecture <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7022b6",
   "metadata": {},
   "source": [
    "Author: Samuel Nordmann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd328d7b",
   "metadata": {},
   "source": [
    "Here we propose a TensorFlow implementation from scratch of the Transformer as described in the original article [Vaswani et. al. Attention is all you need. Advances in Neural Information Processing Systems, 2017]. We give a detailed report on this article and define all the notations in an attached notebook. \n",
    "\n",
    "The details of the implementation follow the same line as  the one proposed by Andrew Ng in his Specialization course on Deep Learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd5812",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Packages\" data-toc-modified-id=\"Packages-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Packages</a></span></li><li><span><a href=\"#Sub-layers\" data-toc-modified-id=\"Sub-layers-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Sub layers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Masking\" data-toc-modified-id=\"Masking-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Masking</a></span><ul class=\"toc-item\"><li><span><a href=\"#Padding-mask\" data-toc-modified-id=\"Padding-mask-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Padding mask</a></span></li><li><span><a href=\"#Look-ahead-Mask\" data-toc-modified-id=\"Look-ahead-Mask-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Look-ahead Mask</a></span></li></ul></li><li><span><a href=\"#Attention\" data-toc-modified-id=\"Attention-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Attention</a></span><ul class=\"toc-item\"><li><span><a href=\"#Single-head-attention\" data-toc-modified-id=\"Single-head-attention-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Single head attention</a></span></li><li><span><a href=\"#MultiHead-Attention\" data-toc-modified-id=\"MultiHead-Attention-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>MultiHead Attention</a></span></li></ul></li><li><span><a href=\"#Pointwise-FNN\" data-toc-modified-id=\"Pointwise-FNN-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Pointwise FNN</a></span></li><li><span><a href=\"#Positional-encoding\" data-toc-modified-id=\"Positional-encoding-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Positional encoding</a></span></li></ul></li><li><span><a href=\"#Encoder\" data-toc-modified-id=\"Encoder-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Encoder</a></span><ul class=\"toc-item\"><li><span><a href=\"#Encoder-layer\" data-toc-modified-id=\"Encoder-layer-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Encoder layer</a></span></li><li><span><a href=\"#Full-Encoder\" data-toc-modified-id=\"Full-Encoder-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Full Encoder</a></span></li></ul></li><li><span><a href=\"#Decoder\" data-toc-modified-id=\"Decoder-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Decoder</a></span><ul class=\"toc-item\"><li><span><a href=\"#Decoder-layer\" data-toc-modified-id=\"Decoder-layer-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Decoder layer</a></span></li><li><span><a href=\"#Full-decoder\" data-toc-modified-id=\"Full-decoder-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Full decoder</a></span></li></ul></li><li><span><a href=\"#Transformer\" data-toc-modified-id=\"Transformer-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Transformer</a></span><ul class=\"toc-item\"><li><span><a href=\"#Compilation-of-the-model\" data-toc-modified-id=\"Compilation-of-the-model-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Compilation of the model</a></span></li></ul></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Learning-rate-scheduler\" data-toc-modified-id=\"Learning-rate-scheduler-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Learning rate scheduler</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6294369",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bd8868dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Dense, Input, Dropout, LayerNormalization, Conv1D, Reshape\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c07e96",
   "metadata": {},
   "source": [
    "# Sub layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc410cd",
   "metadata": {},
   "source": [
    "## Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ede053",
   "metadata": {},
   "source": [
    "There are two types of masks that are useful when building your Transformer network: the padding mask and the look-ahead mask. Both help the softmax computation give the appropriate weights to the words in the input sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc750a52",
   "metadata": {},
   "source": [
    "### Padding mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a03c02f",
   "metadata": {},
   "source": [
    "When passing sequences into a transformer model, it is important that they are of uniform length. You can achieve this by padding the sequence with zeros, and truncating sentences that exceed the maximum length of your model.\n",
    "\n",
    "In the case where the sequence is padded with zeros, we need to create a mask to let the algorithm know that it should discard the zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f322148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(decoder_token_ids):\n",
    "    \"\"\"\n",
    "    Creates a matrix mask for the padding cells\n",
    "    \n",
    "    Arguments:\n",
    "        decoder_token_ids -- (n, m) matrix. n=batch_size, m=fixed size of input sentences\n",
    "    \n",
    "    Returns:\n",
    "        mask -- (n, 1, m) binary tensor\n",
    "    \"\"\"    \n",
    "    seq = 1 - tf.cast(tf.math.equal(decoder_token_ids, 0), tf.float32)\n",
    "  \n",
    "    # add extra dimensions to add the padding to the attention logits.\n",
    "    return seq[:, tf.newaxis, :] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25b16d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1. 1. 0. 0. 1.]]\n",
      "\n",
      " [[1. 1. 1. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 1. 1.]]], shape=(3, 1, 5), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-05 23:42:35.182858: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[7., 6., 0., 0., 1.], [1., 2., 3., 0., 0.], [0., 0., 0., 4., 5.]])\n",
    "print(create_padding_mask(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f468d2e",
   "metadata": {},
   "source": [
    "### Look-ahead Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cc23b2",
   "metadata": {},
   "source": [
    "In training, we have access to the complete correct output of the training example. The look-ahead mask helps the model pretend that it correctly predicted a part of the output and see if, without looking ahead, it can correctly predict the next output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f216f149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(sequence_length):\n",
    "    \"\"\"\n",
    "    Returns an upper triangular matrix filled with ones\n",
    "    \n",
    "    Arguments:\n",
    "        sequence_length -- matrix size\n",
    "    \n",
    "    Returns:\n",
    "        mask -- (size, size) tensor\n",
    "    \"\"\"\n",
    "    mask = tf.linalg.band_part(tf.ones((1, sequence_length, sequence_length)), -1, 0)\n",
    "    return mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09bb8fbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\n",
       "array([[[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "create_look_ahead_mask(x.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17e1dd8",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f12698",
   "metadata": {},
   "source": [
    "### Single head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75f15080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Calculate the attention weights.\n",
    "\n",
    "    Arguments:\n",
    "        Q -- query shape == (..., Tq, dk)\n",
    "        K -- key shape == (..., Tv, dk)\n",
    "        V -- value shape == (..., Tv, dv)\n",
    "        mask: Float tensor with shape broadcastable to (..., Tq, Tv). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        output -- (attention,attention_weights)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Compute the scaled dot-product Q•K\n",
    "    matmul_QK = tf.matmul(Q,K,transpose_b=True)  # dot-product of shape (..., Tq, Tv)\n",
    "\n",
    "    dk = K.shape[-1]\n",
    "    scaled_attention_logits = matmul_QK/np.sqrt(dk) # scaled dot-product of shape (..., Tq, Tv)\n",
    "\n",
    "    # Add the mask to the scaled dot-product\n",
    "    if mask is not None: \n",
    "        scaled_attention_logits += (1. - mask) *(-1e9)\n",
    "\n",
    "    # Compute the Softmax\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # weights of shape (..., Tq, Tv)\n",
    "\n",
    "    #Multiply with V\n",
    "    output = tf.matmul(attention_weights,V)  # Attention representation of shape (..., Tq, dv)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1a838ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 7, 64)\n",
      "(16, 9, 128)\n"
     ]
    }
   ],
   "source": [
    "## DEBUG\n",
    "\n",
    "#Withtout masking\n",
    "batch_size,Tq, Tv, dk, dv= 8,7,9,32,64\n",
    "Q= tf.random.uniform((batch_size,Tq, dk))\n",
    "K= tf.random.uniform((batch_size,Tv, dk))\n",
    "V= tf.random.uniform((batch_size,Tv, dv))\n",
    "\n",
    "A,_=scaled_dot_product_attention(Q, K, V)\n",
    "print(A.shape)\n",
    "\n",
    "#With masking. Needs Tq=Tv\n",
    "batch_size,Tq, Tv, dk, dv= 16,9,9,64,128\n",
    "Q= tf.random.uniform((batch_size,Tq, dk))\n",
    "K= tf.random.uniform((batch_size,Tv, dk))\n",
    "V= tf.random.uniform((batch_size,Tv, dv))\n",
    "mask=create_look_ahead_mask(Tq)\n",
    "\n",
    "A,_=scaled_dot_product_attention(Q, K, V)\n",
    "print(A.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e58815",
   "metadata": {},
   "source": [
    "### MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12e49cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multihead_Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, H, d_model, dk, dv):\n",
    "        \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        H -- number of heads (=8 in the paper)\n",
    "        d_models -- embedding dimension (=512 in the paper)\n",
    "        dk -- depth of Q and K (=64 in the paper)\n",
    "        dv -- depth of V (=64 in the paper)\n",
    "        \"\"\"\n",
    "    \n",
    "        super(Multihead_Attention, self).__init__()\n",
    "        \n",
    "        initializer = tf.keras.initializers.GlorotUniform()\n",
    "        self.WQ = tf.Variable(initializer(shape=(H, d_model, dk)), trainable=True)\n",
    "        self.WK = tf.Variable(initializer(shape=(H, d_model, dk)), trainable=True)\n",
    "        self.WV = tf.Variable(initializer(shape=(H, d_model, dv)), trainable=True)\n",
    "        self.WO = tf.Variable(initializer(shape=(H*dv,d_model)), trainable=True)\n",
    "\n",
    "    \n",
    "    def call(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Calculate the attention weights.\n",
    "\n",
    "        Arguments:\n",
    "            Q -- query shape == (..., Tq, d_model)\n",
    "            K -- key shape == (..., Tv, d_model)\n",
    "            V -- value shape == (..., Tv, d_model)\n",
    "            mask: Float tensor with shape broadcastable to (..., Tq, Tv). Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            output -- Multihead attention A of shape (batch_size, Tq, d_model)\n",
    "        \"\"\"\n",
    "        #Projecting Q,K,V to Qh, Kh, Vh. The H projection are stacked on the penultiem axis\n",
    "        Qh= tf.experimental.numpy.dot(Q, self.WQ) #of shape (batch_size, Tq, H, dk)\n",
    "        Kh= tf.experimental.numpy.dot(K, self.WK) #of shape (batch_size, Tv, H, dk)\n",
    "        Vh= tf.experimental.numpy.dot(V, self.WV) #of shape (batch_size, Tv, H, dv)\n",
    "        \n",
    "        #Transposition\n",
    "        Qh=tf.transpose(Qh, [0,2,1,3]) #of shape (batch_size, H, Tq, dk)\n",
    "        Kh=tf.transpose(Kh, [0,2,1,3]) #of shape (batch_size, H, Tv, dk)\n",
    "        Vh=tf.transpose(Vh, [0,2,1,3]) #of shape (batch_size, H, Tv, dv)\n",
    "        \n",
    "        # Computing the dot-product attention\n",
    "        Ah,_=scaled_dot_product_attention(Qh, Kh, Vh, mask=mask) #of shape (batch_size, H, Tq, dv)\n",
    "        \n",
    "        #Flattening the H and dv axis and projecting back to d_model\n",
    "#        A = tf.reshape(Ah,(*Ah.shape[:-2],-1))\n",
    "        s=Ah.shape\n",
    "        A = tf.reshape(Ah,(s[0],s[2],s[1]*s[3])) #of shape (batch_size, Tq, H*dv)\n",
    "        A= tf.experimental.numpy.dot(A, self.WO) #of shape (batch_size, Tq, d_model)\n",
    "        \n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55e12527",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 9, 512)\n"
     ]
    }
   ],
   "source": [
    "## DEBUG\n",
    "\n",
    "H, d_model, dk, dv=8,512,64,32\n",
    "batch_size, Tq, Tv = 16,9,9\n",
    "\n",
    "mha_layer= Multihead_Attention(H, d_model, dk, dv)\n",
    "\n",
    "Q= tf.random.uniform((batch_size, Tq, d_model))\n",
    "K= tf.random.uniform((batch_size, Tv, d_model))\n",
    "V= tf.random.uniform((batch_size, Tv, d_model))\n",
    "mask=create_look_ahead_mask(Tq)\n",
    "\n",
    "A=mha_layer(Q,K,V)\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8468ef79",
   "metadata": {},
   "source": [
    "## Pointwise FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b3048ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNNLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff):\n",
    "        \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        d_model -- the dimension of the embedding (=64 in the paper)\n",
    "        dff -- the dimension of the hidden layer of the FNN (=2048 in the paper)\n",
    "        \"\"\"\n",
    "            \n",
    "        super(FNNLayer, self).__init__()\n",
    "\n",
    "        self.layer1 = Conv1D(filters=dff, kernel_size=1,activation=\"relu\")\n",
    "        self.layer2 = Conv1D(filters=d_model, kernel_size=1)\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
    "            \n",
    "        Returns:\n",
    "            fnn_layer_out -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        x=self.layer1(x)\n",
    "        fnn_layer_out=self.layer2(x)\n",
    "\n",
    "        \n",
    "        return fnn_layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba78f4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 9, 64)\n"
     ]
    }
   ],
   "source": [
    "## DEBUG\n",
    "\n",
    "d_model, dff = 64, 2048\n",
    "fnn_layer= FNNLayer(d_model, dff)\n",
    "\n",
    "batch_size, Tv= 16, 9\n",
    "x=tf.random.uniform((batch_size, Tv, d_model))\n",
    "\n",
    "print(fnn_layer(x).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dee17c3",
   "metadata": {},
   "source": [
    "## Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e90df3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(positions, d):\n",
    "    \"\"\"\n",
    "    Precomputes a matrix with all the positional encodings \n",
    "    \n",
    "    Arguments:\n",
    "        positions (int) -- Maximum number of positions to be encoded \n",
    "        d (int) -- Encoding size d_model\n",
    "    \n",
    "        arguments de get_angles:\n",
    "            pos -- Column vector containing the positions [[0], [1], ...,[N-1]]\n",
    "            k --   Row vector containing the dimension span [[0, 1, 2, ..., d-1]]\n",
    "            d(integer) -- Encoding size\n",
    "            \n",
    "    Returns:\n",
    "        pos_encoding -- (1, position, d_model) A matrix with the positional encodings\n",
    "    \"\"\"\n",
    "    # initialize a matrix angle_rads of all the angles\n",
    "    pos=np.arange(positions)[:, np.newaxis] #Column vector containing the position span [0,1,..., positions]\n",
    "    k= np.arange(d)[np.newaxis, :]  #Row vector containing the dimension span [[0, 1, ..., d-1]]\n",
    "    i = k//2\n",
    "    angle_rads = pos/(10000**(2*i/d)) #Matrix of angles indexed by (pos,i)\n",
    "    \n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    #adds batch axis\n",
    "    pos_encoding = angle_rads[np.newaxis, ...] \n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58723889",
   "metadata": {},
   "source": [
    "Let us plot the obtained results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9278ee6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "positions, d=50,512\n",
    "pos_encoding = positional_encoding(positions, d)\n",
    "\n",
    "print (pos_encoding.shape)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('d')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585ca3b",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfca8d4",
   "metadata": {},
   "source": [
    "## Encoder layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf62a7b",
   "metadata": {},
   "source": [
    "We recall that the encoder layer is composed by a multi-head self-attention mechanism, followed by a positionwise fully connected feed-forward network. This archirecture includes a residual connection around each of the two sub-layers, followed by layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "beeb3119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, H, d_model, dk, dv, dff, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "\n",
    "        H -- number of heads (=8 in the paper)\n",
    "        d_models -- embedding dimension (=512 in the paper)\n",
    "        dk -- depth of Q and K (=64 in the paper)\n",
    "        dv -- depth of V (=64 in the paper)\n",
    "        dff -- the dimension of the hidden layer of the FNN (=2048 in the paper)\n",
    "        dropout_rate -- Dropout parameter used (during training) before all the residual connections\n",
    "        layernorm_eps -- eta regularizing parameter for the Normalization layer \n",
    "        \"\"\"\n",
    "        \n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha = Multihead_Attention(H, d_model, dk, dv)\n",
    "        self.ffn = FNNLayer(d_model, dff)\n",
    "        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.dropout_mha = Dropout(dropout_rate)\n",
    "        self.dropout_ffn = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, training=False, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder Layer\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, Tq, d_model)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers. Defaults to False\n",
    "            mask -- Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input. Defaults to None\n",
    "        Returns:\n",
    "            encoder_layer_out -- Tensor of shape (batch_size, Tq, d_model)\n",
    "        \"\"\"\n",
    "        A = self.mha(x,x,x,mask=mask) # Self attention (batch_size, Tq, d_model)\n",
    "        A = self.dropout_mha(A, training=training) #Apply Dropout during training\n",
    "        \n",
    "        \n",
    "        #  Residual connection + Layer normalization\n",
    "        out1 = self.layernorm1(x+A)  # (batch_size, Tq, d_model)\n",
    "\n",
    "        # Pointwise ffn\n",
    "        ffn_output = self.ffn(out1) # (batch_size, Tq, d_model)\n",
    "        ffn_output = self.dropout_ffn(ffn_output, training=training) # Apply Dropout during training\n",
    "        \n",
    "        # Residual connection + Layer normalization\n",
    "        encoder_layer_out = self.layernorm2(ffn_output+out1)  # (batch_size, input_seq_len, fully_connected_dim)\n",
    "        \n",
    "        return encoder_layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73ff40db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43, 27, 512)\n"
     ]
    }
   ],
   "source": [
    "## DEBUG\n",
    "\n",
    "H, d_model, dk, dv, dff = 8, 512, 64, 32, 2048\n",
    "layer = EncoderLayer(H, d_model, dk, dv, dff)\n",
    "\n",
    "batch_size, Tq= 43, 27\n",
    "x = tf.random.uniform((batch_size, Tq, d_model))\n",
    "\n",
    "output = layer(x,training=True)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332666b5",
   "metadata": {},
   "source": [
    "## Full Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "add7de88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, N, H, d_model, dk, dv, dff, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        \n",
    "        N -- number of stackeds Encoder layers (=6 in the paper)\n",
    "        H -- number of heads (=8 in the paper)\n",
    "        d_models -- embedding dimension (=512 in the paper)\n",
    "        dk -- depth of Q and K (=64 in the paper)\n",
    "        dv -- depth of V (=64 in the paper)\n",
    "        dff -- the dimension of the hidden layer of the FNN (=2048 in the paper)\n",
    "        dropout_rate -- Dropout parameter used (during training) before all the residual connections\n",
    "        layernorm_eps -- eta regularizing parameter for the Normalization layer \n",
    "        \"\"\"\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.layers=[EncoderLayer(H, d_model, dk, dv, dff, \n",
    "                                  dropout_rate=dropout_rate, \n",
    "                                  layernorm_eps=layernorm_eps)\n",
    "                                  for i in range(N)]\n",
    "    \n",
    "    def call(self, x, training=False, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, Tq, d_model)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers. Defaults to False\n",
    "            mask -- Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input. Defaults to None\n",
    "        Returns:\n",
    "            encoder_out -- Tensor of shape (batch_size, Tq, d_model)\n",
    "        \"\"\"\n",
    "                                  \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, training=training, mask=mask)\n",
    "                                  \n",
    "        return x\n",
    "                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a70af32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43, 27, 512)\n"
     ]
    }
   ],
   "source": [
    "## DEBUG\n",
    "\n",
    "N,H, d_model, dk, dv, dff = 6,8, 512, 64, 32, 2048\n",
    "encoder = Encoder(N,H, d_model, dk, dv, dff)\n",
    "\n",
    "batch_size, Tq= 43, 27\n",
    "x = tf.random.uniform((batch_size, Tq, d_model))\n",
    "\n",
    "output = encoder(x,training=True)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663ab88f",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba26cd7c",
   "metadata": {},
   "source": [
    "## Decoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "509b3ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, H, d_model, dk, dv, dff, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "\n",
    "        H -- number of heads (=8 in the paper)\n",
    "        d_models -- embedding dimension (=512 in the paper)\n",
    "        dk -- depth of Q and K (=64 in the paper)\n",
    "        dv -- depth of V (=64 in the paper)\n",
    "        dff -- the dimension of the hidden layer of the FNN (=2048 in the paper)\n",
    "        dropout_rate -- Dropout parameter used (during training) before all the residual connections\n",
    "        layernorm_eps -- eta regularizing parameter for the Normalization layer \n",
    "        \"\"\"\n",
    "        \n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha1 = Multihead_Attention(H, d_model, dk, dv)\n",
    "        self.mha2 = Multihead_Attention(H, d_model, dk, dv)\n",
    "        self.ffn = FNNLayer(d_model, dff)\n",
    "        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm3 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.dropout_mha1 = Dropout(dropout_rate)\n",
    "        self.dropout_mha2 = Dropout(dropout_rate)                                     \n",
    "        self.dropout_ffn = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, encoder_output, training=False, look_ahead_mask=None, padding_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the Decoder Layer\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, Tv, d_model)\n",
    "            encoder_output --  Tensor of shape (batch_size, Tv, d_model)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers. Defaults to False\n",
    "            look_ahead_mask -- Boolean mask for the target_input. Defaults to None\n",
    "            padding_mask -- Boolean mask for the second multihead attention layer. Defaults to None\n",
    "        Returns:\n",
    "            decoder_layer_out -- Tensor of shape (batch_size, Tq, d_model)\n",
    "        \"\"\"\n",
    "        # 1st Masked MultiHead attention                                     \n",
    "        A1 = self.mha1(x,x,x,mask=look_ahead_mask) # Self attention (batch_size, Tq, d_model)\n",
    "        A1 = self.dropout_mha1(A1, training=training) #Apply Dropout during training\n",
    "        \n",
    "        #  Residual connection + Layer normalization\n",
    "        out1 = self.layernorm1(x+A1)  # (batch_size, Tq, d_model)\n",
    "\n",
    "        # 2nd Masked MultiHead attention                                     \n",
    "        A2 = self.mha2(x,encoder_output,encoder_output,mask=padding_mask) # Self attention (batch_size, Tq, d_model)\n",
    "        A2 = self.dropout_mha2(A2, training=training) #Apply Dropout during training\n",
    "        \n",
    "        \n",
    "        #  Residual connection + Layer normalization\n",
    "        out2 = self.layernorm2(out1+A2)  # (batch_size, Tq, d_model)\n",
    "                                             \n",
    "        # Pointwise ffn\n",
    "        ffn_output = self.ffn(out2) # (batch_size, Tq, d_model)\n",
    "        ffn_output = self.dropout_ffn(ffn_output, training=training) # Apply Dropout during training\n",
    "        \n",
    "        # Residual connection + Layer normalization\n",
    "        decoder_layer_out = self.layernorm3(ffn_output+out2)  # (batch_size, input_seq_len, fully_connected_dim)\n",
    "        \n",
    "        return decoder_layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b34cf916",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEBUG\n",
    "\n",
    "H, d_model, dk, dv, dff = 8, 512, 64, 32, 2048\n",
    "layer = DecoderLayer(H, d_model, dk, dv, dff)\n",
    "\n",
    "batch_size, Tq, Tv= 43, 57, 57\n",
    "x = tf.random.uniform((batch_size, Tv, d_model))\n",
    "encoder_output = tf.random.uniform((batch_size, Tq, d_model))\n",
    "\n",
    "output = layer(x,encoder_output,training=True)\n",
    "#print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913d112f",
   "metadata": {},
   "source": [
    "## Full decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9277f5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, N, H, d_model, dk, dv, dff, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "\n",
    "        N -- number of stackeds Decoder layers (=6 in the paper)\n",
    "        H -- number of heads (=8 in the paper)\n",
    "        d_models -- embedding dimension (=512 in the paper)\n",
    "        dk -- depth of Q and K (=64 in the paper)\n",
    "        dv -- depth of V (=64 in the paper)\n",
    "        dff -- the dimension of the hidden layer of the FNN (=2048 in the paper)\n",
    "        dropout_rate -- Dropout parameter used (during training) before all the residual connections\n",
    "        layernorm_eps -- eta regularizing parameter for the Normalization layer \n",
    "        \"\"\"\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.layers=[DecoderLayer(H, d_model, dk, dv, dff, \n",
    "                                  dropout_rate=dropout_rate, \n",
    "                                  layernorm_eps=layernorm_eps)\n",
    "                                  for i in range(N)]\n",
    "    \n",
    "    def call(self, x, encoder_output, training=False, look_ahead_mask=None, padding_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the Decoder Layer\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, Tv, d_model)\n",
    "            encoder_output --  Tensor of shape (batch_size, Tv, d_model)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers. Defaults to False\n",
    "            look_ahead_mask -- Boolean mask for the target_input. Defaults to None\n",
    "            padding_mask -- Boolean mask for the second multihead attention layer. Defaults to None\n",
    "        Returns:\n",
    "            decoder_out -- Tensor of shape (batch_size, Tq, d_model)\n",
    "        \"\"\"\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x,encoder_output, look_ahead_mask=look_ahead_mask, padding_mask=padding_mask)\n",
    "                                  \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c3f676d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43, 57, 512)\n",
      "(43, 57, 512)\n",
      "(43, 57, 512)\n",
      "(43, 57, 512)\n",
      "(43, 57, 512)\n",
      "(43, 57, 512)\n",
      "(43, 57, 512)\n",
      "(43, 57, 512)\n",
      "(43, 57, 512)\n",
      "(43, 57, 512)\n",
      "(43, 57, 512)\n",
      "(43, 57, 512)\n",
      "(43, 57, 512)\n",
      "(43, 57, 512)\n",
      "(43, 57, 512)\n",
      "(43, 57, 512)\n",
      "(43, 57, 512)\n",
      "(43, 57, 512)\n",
      "(43, 57, 512)\n"
     ]
    }
   ],
   "source": [
    "## DEBUG\n",
    "\n",
    "N,H, d_model, dk, dv, dff = 6,8, 512, 64, 32, 2048\n",
    "decoder = Decoder(N,H, d_model, dk, dv, dff)\n",
    "\n",
    "batch_size, Tq, Tv= 43, 57, 57\n",
    "x = tf.random.uniform((batch_size, Tv, d_model))\n",
    "encoder_output = tf.random.uniform((batch_size, Tq, d_model))\n",
    "\n",
    "output = decoder(x,encoder_output,training=True)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdd84b9",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8f12cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, N, H, d_model, dk, dv, dff, \n",
    "                 vocab_size, max_positional_encoding, \n",
    "                 dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        initializer = tf.keras.initializers.GlorotUniform()\n",
    "        self.embedding = tf.Variable(initializer(shape=(vocab_size, d_model)), trainable=True)\n",
    "        self.PE = positional_encoding(max_positional_encoding, d_model)\n",
    "        \n",
    "        self.dropout_encoding_input = Dropout(dropout_rate)\n",
    "        self.dropout_decoding_input = Dropout(dropout_rate)\n",
    "        \n",
    "        self.encoder = Encoder(N, H, d_model, dk, dv, dff, dropout_rate=dropout_rate, layernorm_eps=layernorm_eps)\n",
    "        self.decoder = Decoder(N, H, d_model, dk, dv, dff, dropout_rate=dropout_rate, layernorm_eps=layernorm_eps)\n",
    "\n",
    "        \n",
    "\n",
    "    def call(self, x, y, training=False, enc_padding_mask=None, look_ahead_mask=None, dec_padding_mask=None):\n",
    "        \n",
    "        x = tf.matmul(x,self.embedding)\n",
    "        x = x + self.PE\n",
    "        x =  self.dropout_encoding_input(x,training=training)\n",
    "        \n",
    "        encoder_output = self.encoder(x,training=training, mask=enc_padding_mask)\n",
    "        \n",
    "        y = tf.matmul(y,self.embedding)\n",
    "        y = y + self.PE\n",
    "        y = self.dropout_decoding_input(y,training=training)\n",
    "        \n",
    "        dec_output = self.decoder(y, encoder_output, training=training, \n",
    "                                  look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask)\n",
    "        \n",
    "        \n",
    "        pred =  tf.matmul(embedding,dec_output,transpose_b=True)\n",
    "        pred = tf.nn.softmax(pred)\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "00cdee90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 29, 11)\n",
      "Model: \"transformer_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dropout_533 (Dropout)       multiple                  0         \n",
      "                                                                 \n",
      " dropout_534 (Dropout)       multiple                  0         \n",
      "                                                                 \n",
      " encoder_15 (Encoder)        multiple                  18902016  \n",
      "                                                                 \n",
      " decoder_17 (Decoder)        multiple                  25199616  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,116,480\n",
      "Trainable params: 44,116,480\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "N, H, d_model, dk, dv, dff = 6, 8, 512, 64, 64, 2048\n",
    "vocab_size, T =29, 11\n",
    "batch_size = 3\n",
    "\n",
    "transformer = Transformer(N, H, d_model, dk, dv, dff, \n",
    "                 vocab_size, max_positional_encoding)\n",
    "\n",
    "input_shape = (None, T,vocab_size)\n",
    "\n",
    "\n",
    "x = tf.random.uniform((batch_size, T, vocab_size))\n",
    "y =  tf.random.uniform((batch_size, T, vocab_size))\n",
    "\n",
    "pred = transformer(x,y,training=True)\n",
    "print(pred.shape)\n",
    "\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e454268b",
   "metadata": {},
   "source": [
    "## Compilation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "713bfaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0, beta_1=0.9, beta_2=0.98, epsilon=1e-09)\n",
    "\n",
    "transformer.compile(loss='crossentropy',optimizer=optimizer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8cc30b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0778a6",
   "metadata": {},
   "source": [
    "## Learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ee9d4880",
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_step = 4000\n",
    "class LearningRateScheduler(tf.keras.callbacks.Callback):\n",
    "    def on_train_batch_start(self, i, batch_logs):\n",
    "        transformer.optimizer.lr = dk**(-0.5)*min(i**(-0.5),warmup_step**(-3/2)*i)\n",
    "\n",
    "\n",
    "callback = LearningRateScheduler()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 409,
   "position": {
    "height": "431px",
    "left": "735px",
    "right": "20px",
    "top": "-20px",
    "width": "354px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
