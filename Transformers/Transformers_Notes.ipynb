{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed329c8d",
   "metadata": {},
   "source": [
    "# Transformers for automated translation <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7fa203",
   "metadata": {},
   "source": [
    "Author: Samuel Nordmann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c9cb51",
   "metadata": {},
   "source": [
    "In this notebook, I report on the article [Attention is All you Need, Vaswani et. al. (2017)] where the Transformer architecture has been introduced. To fix ideas, the discussion is focus on the task of automated translation between two languages, yet, transformers can be applied to many other contexts of NLP or Series analysis.\n",
    "\n",
    "These notes were first intended for personal use, but I decided to share them in the hope that it could help others.\n",
    "\n",
    "We first present the background theoretical framework and classical existing model, before presenting the details of the Transformer architecture.\n",
    "\n",
    "We also propose in an attached notebook a TensforFlow implementation from scratch of the Transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d300d",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preliminary:-general-introduction-and-background-on-translation-machines.\" data-toc-modified-id=\"Preliminary:-general-introduction-and-background-on-translation-machines.-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preliminary: general introduction and background on translation machines.</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-framework-of-machine-translation\" data-toc-modified-id=\"The-framework-of-machine-translation-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>The framework of machine translation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Probabilistic-model\" data-toc-modified-id=\"Probabilistic-model-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Probabilistic model</a></span></li><li><span><a href=\"#Auto-Regression-and-Beam-Search\" data-toc-modified-id=\"Auto-Regression-and-Beam-Search-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Auto Regression and Beam Search</a></span></li></ul></li><li><span><a href=\"#Common-algorithmic-architectures\" data-toc-modified-id=\"Common-algorithmic-architectures-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Common algorithmic architectures</a></span><ul class=\"toc-item\"><li><span><a href=\"#Encoder/Decoder-structure\" data-toc-modified-id=\"Encoder/Decoder-structure-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Encoder/Decoder structure</a></span></li><li><span><a href=\"#Reccurent-Neural-Networks\" data-toc-modified-id=\"Reccurent-Neural-Networks-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Reccurent Neural Networks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simple-RNN\" data-toc-modified-id=\"Simple-RNN-1.2.2.1\"><span class=\"toc-item-num\">1.2.2.1&nbsp;&nbsp;</span>Simple RNN</a></span></li><li><span><a href=\"#Gated-Recurrent-Unit-(GRU)\" data-toc-modified-id=\"Gated-Recurrent-Unit-(GRU)-1.2.2.2\"><span class=\"toc-item-num\">1.2.2.2&nbsp;&nbsp;</span>Gated Recurrent Unit (GRU)</a></span></li><li><span><a href=\"#Long-Short-Term-Memory-unit-(LSTM)\" data-toc-modified-id=\"Long-Short-Term-Memory-unit-(LSTM)-1.2.2.3\"><span class=\"toc-item-num\">1.2.2.3&nbsp;&nbsp;</span>Long Short Term Memory unit (LSTM)</a></span></li></ul></li><li><span><a href=\"#additive-Attention-mechanism\" data-toc-modified-id=\"additive-Attention-mechanism-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>additive Attention mechanism</a></span></li></ul></li><li><span><a href=\"#Limits-of-the-common-architectures-and-motivations-for-the-Transformer-architecture\" data-toc-modified-id=\"Limits-of-the-common-architectures-and-motivations-for-the-Transformer-architecture-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Limits of the common architectures and motivations for the Transformer architecture</a></span></li></ul></li><li><span><a href=\"#the-Transformer-algorithm\" data-toc-modified-id=\"the-Transformer-algorithm-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>the Transformer algorithm</a></span><ul class=\"toc-item\"><li><span><a href=\"#Overview-of-the-architecture\" data-toc-modified-id=\"Overview-of-the-architecture-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Overview of the architecture</a></span></li><li><span><a href=\"#Description-of-the-layers\" data-toc-modified-id=\"Description-of-the-layers-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Description of the layers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Multi-head-attention\" data-toc-modified-id=\"Multi-head-attention-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Multi-head attention</a></span><ul class=\"toc-item\"><li><span><a href=\"#Scaled-dot-product-Attention\" data-toc-modified-id=\"Scaled-dot-product-Attention-2.2.1.1\"><span class=\"toc-item-num\">2.2.1.1&nbsp;&nbsp;</span>Scaled dot-product Attention</a></span></li><li><span><a href=\"#Multi-head-Attention\" data-toc-modified-id=\"Multi-head-Attention-2.2.1.2\"><span class=\"toc-item-num\">2.2.1.2&nbsp;&nbsp;</span>Multi-head Attention</a></span></li><li><span><a href=\"#Dimensions\" data-toc-modified-id=\"Dimensions-2.2.1.3\"><span class=\"toc-item-num\">2.2.1.3&nbsp;&nbsp;</span>Dimensions</a></span></li><li><span><a href=\"#Self-Attention\" data-toc-modified-id=\"Self-Attention-2.2.1.4\"><span class=\"toc-item-num\">2.2.1.4&nbsp;&nbsp;</span>Self Attention</a></span></li><li><span><a href=\"#Comparison-with-the-classical-previous-models\" data-toc-modified-id=\"Comparison-with-the-classical-previous-models-2.2.1.5\"><span class=\"toc-item-num\">2.2.1.5&nbsp;&nbsp;</span>Comparison with the classical previous models</a></span></li><li><span><a href=\"#Use-of-Multi-head-attention-in-the-Transformer-[sec:UtilizationAttention]\" data-toc-modified-id=\"Use-of-Multi-head-attention-in-the-Transformer-[sec:UtilizationAttention]-2.2.1.6\"><span class=\"toc-item-num\">2.2.1.6&nbsp;&nbsp;</span>Use of Multi-head attention in the Transformer <a class=\"latex_label_anchor\" id=\"secUtilizationAttention\" rel=\"nofollow\" style=\"display: none;\">[sec:UtilizationAttention]</a><a id=\"secUtilizationAttention_\" rel=\"nofollow\"></a></a></span></li></ul></li><li><span><a href=\"#(Point-wise)-Feed-forward-NN\" data-toc-modified-id=\"(Point-wise)-Feed-forward-NN-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>(Point-wise) Feed forward NN</a></span></li><li><span><a href=\"#Embeddings-and-Softmax-layers\" data-toc-modified-id=\"Embeddings-and-Softmax-layers-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Embeddings and Softmax layers</a></span></li><li><span><a href=\"#Positional-encoding\" data-toc-modified-id=\"Positional-encoding-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>Positional encoding</a></span></li><li><span><a href=\"#Normalization-layer\" data-toc-modified-id=\"Normalization-layer-2.2.5\"><span class=\"toc-item-num\">2.2.5&nbsp;&nbsp;</span>Normalization layer</a></span></li></ul></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loss\" data-toc-modified-id=\"Loss-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Loss</a></span></li><li><span><a href=\"#Optimizer\" data-toc-modified-id=\"Optimizer-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Optimizer</a></span><ul class=\"toc-item\"><li><span><a href=\"#Adam-Optimizer\" data-toc-modified-id=\"Adam-Optimizer-2.3.2.1\"><span class=\"toc-item-num\">2.3.2.1&nbsp;&nbsp;</span>Adam Optimizer</a></span></li><li><span><a href=\"#Learning-rate-scheduler\" data-toc-modified-id=\"Learning-rate-scheduler-2.3.2.2\"><span class=\"toc-item-num\">2.3.2.2&nbsp;&nbsp;</span>Learning rate scheduler</a></span></li><li><span><a href=\"#Choice-for-the-learning-hyperparameters\" data-toc-modified-id=\"Choice-for-the-learning-hyperparameters-2.3.2.3\"><span class=\"toc-item-num\">2.3.2.3&nbsp;&nbsp;</span>Choice for the learning hyperparameters</a></span></li></ul></li><li><span><a href=\"#Regularization\" data-toc-modified-id=\"Regularization-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>Regularization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dropout\" data-toc-modified-id=\"Dropout-2.3.3.1\"><span class=\"toc-item-num\">2.3.3.1&nbsp;&nbsp;</span>Dropout</a></span></li><li><span><a href=\"#Label-smoothing\" data-toc-modified-id=\"Label-smoothing-2.3.3.2\"><span class=\"toc-item-num\">2.3.3.2&nbsp;&nbsp;</span>Label smoothing</a></span></li></ul></li><li><span><a href=\"#Dataset-and-batch\" data-toc-modified-id=\"Dataset-and-batch-2.3.4\"><span class=\"toc-item-num\">2.3.4&nbsp;&nbsp;</span>Dataset and batch</a></span></li><li><span><a href=\"#Hardware\" data-toc-modified-id=\"Hardware-2.3.5\"><span class=\"toc-item-num\">2.3.5&nbsp;&nbsp;</span>Hardware</a></span></li></ul></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Evaluation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Metrics\" data-toc-modified-id=\"Metrics-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Metrics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Accuracy\" data-toc-modified-id=\"Accuracy-2.4.1.1\"><span class=\"toc-item-num\">2.4.1.1&nbsp;&nbsp;</span>Accuracy</a></span></li><li><span><a href=\"#BLEU-score\" data-toc-modified-id=\"BLEU-score-2.4.1.2\"><span class=\"toc-item-num\">2.4.1.2&nbsp;&nbsp;</span>BLEU score</a></span></li></ul></li><li><span><a href=\"#Performance\" data-toc-modified-id=\"Performance-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Performance</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb761106",
   "metadata": {},
   "source": [
    "# Preliminary: general introduction and background on translation machines. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c4a20d",
   "metadata": {},
   "source": [
    "## Notations <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e021f3",
   "metadata": {},
   "source": [
    "- $V$ the vocabulary of the considered language pair (e.g. French/Englisg), containing $\\vert V\\vert$ words\n",
    "- $x=(x^{<1>},\\dots,x^{<T_x>})$ the input sentence composed of $T_x$ words of $V$\n",
    "- $y=(y^{<1>},\\dots,y^{<T_y>})$ the output sequence composed of $T_y$ words of $V$\n",
    "\n",
    "Each word of the vocabulary, represented by a token, is encoded as a one-hot vector of dimension $\\vert V\\vert$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be0da04",
   "metadata": {},
   "source": [
    "## The framework of machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c1c395",
   "metadata": {},
   "source": [
    "### Probabilistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c50bd2c",
   "metadata": {},
   "source": [
    "From a probabilistic perspective, the task of translation reduces to estimating\n",
    "\\begin{equation*}\n",
    "p(y\\vert x)\n",
    "\\end{equation*}\n",
    "which is the probability of $y$ being a good translation given an input $x$. The best prediction $\\hat y$ sequence maximizes the above probability, i.e.,\n",
    "\\begin{equation*}\n",
    "\\hat y =\\underset{y}{\\mathrm{argmax}} p(y\\vert x).\n",
    "\\end{equation*}\n",
    "Since a sentence $y$ is a list of word, it is useful to decompose the probability $p(y\\vert x)$ as\n",
    "\\begin{equation*}\n",
    "p(y\\vert x)= p(y^{<1>}\\vert x)\\ \\times p(y^{<2>}\\vert x,y^{<1>})\\ \\times\\ \\cdots\\ \\times\\  p(y^{<T_y>}\\vert x,y^{<1>},\\dots,y^{<T_y-1>}).\n",
    "\\end{equation*}\n",
    "This way, estimating $p(y\\vert x)$ can be broken into estimating\n",
    "\\begin{equation}\\label{conditional_proba}\n",
    "p(y^{<t>}\\vert x,y^{<1>},\\dots,y^{<t-1>}),\\  \\text{$\\quad$for all $1\\le t\\le T_y$}. \n",
    "\\end{equation}    \n",
    "In other words, the task of translation breaks down to predicting the right next word $y^{<t>}$ given an input $x$ and the preceding words $(y^{<1>},\\dots,y^{<t-1>})$.\n",
    "The best prediction $\\hat y$ thus satisfies\n",
    "\\begin{equation}\\label{y_argmax}\n",
    "\\hat y= (\\hat y^{<1>},\\dots, \\hat y^{<T_y>})=\\underset{ y^{<1>},\\dots,  y^{<T_y>}}{\\mathrm{argmax}} \\prod_{t=1}^{T_y} p(y^{<t>}\\vert x,y^{<1>},\\dots,y^{<t-1>}).\n",
    "\\end{equation}\n",
    "\n",
    "    \n",
    "\\textbf{Remarks.}\n",
    "    \\begin{enumerate}\n",
    "   \\item In order to take into account outputs of variable lengths, it is convenient to use special token \"EOS\" indicating the end of the sentence, and discard all time subsequent to this token. This way, we can choose the length of the input and output sequence fixed and equal $T_x=T_y=T$, by padding shorter sequences and truncating longer ones.\n",
    "    \n",
    "   \\item Since any probability should be strictly less than one, we see from the above decomposition that the more terms in the product, the lesser the overall probability will be. In other words, longer sentences are penalized over short ones. To solve this issue, it is actually preferable to consider the normalized probability\n",
    "\\begin{equation*}\n",
    "p_{norm}(y\\vert x)= p(y\\vert x)^{\\frac{1}{T_y^\\alpha}},\n",
    "\\end{equation*}\n",
    "$\\alpha$ being a hyperparameter (in the paper $\\alpha=0.6$). For simplicity, we omit this refinement in the sequel.\n",
    "    \\end{enumerate}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19183acf",
   "metadata": {},
   "source": [
    "### Auto Regression and Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dec1c50",
   "metadata": {},
   "source": [
    "Assuming that an algorithm can successfully estimate \\eqref{conditional_proba} and thus predict the next word $\\hat y^{<t>}$ that maximizes this conditional probability, we still have to design a way of predicting the whole sequence $\\hat y=(\\hat y^{<1>},\\dots y^{<T_y>})$ which is the solution of \\eqref{y_argmax})."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e58366",
   "metadata": {},
   "source": [
    "#### Exhaustive Search <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f694bec",
   "metadata": {},
   "source": [
    "The computational cost for searching the optimal solution $\\hat y$ of \\eqref{y_argmax} increases exponentially with the length of the output sequence $T_y$. Indeed, since every possible sentence has to be checked, the computational cost of the search is $O(\\vert V\\vert^{T_y})$. \n",
    "\n",
    "For this reason, we cannot use an exhaustive search in practice. To keep the computational cost reasonnable, we have to design an approximate search algorithm, which might output an (hopefully good enough) under-optimal solution but which will be computationnaly cheaper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a992d20",
   "metadata": {},
   "source": [
    "#### Greedy Search Algorithm <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88dec56",
   "metadata": {},
   "source": [
    "The simplest approximate search algorithm is to generate the output sequence $\\hat y$ by using a sequential greedy Auto Regressive algorithm: the $t^{\\text{th}}$ word of the output sentence is chosen as to maximize the probability conditonnaly with respect to the input and the previous words generated so far, i.e.,\n",
    "\\begin{equation*}\n",
    "\\hat y^{<t>}= \\mathrm{argmax}_y p(y\\vert x, \\hat y^{<0>},\\dots,\\hat y^{<t-1>}),\\text{$\\quad$ for all $1\\le t \\le T_y$} \n",
    "\\end{equation*}\n",
    "The term \\textit{Auto Regressive} refers to the fact that the previously generated outputs $\\hat y^{<0>},\\dots,\\hat y^{<t-1>}$ are used in the choice of the next output $\\hat y^{<t>}$.\n",
    "    The computational cost of this search algorithm is $O(\\vert V\\vert T_y)$ and is therefore linear. \n",
    "    \n",
    "\n",
    "In practice however, this greedy approach may give poor performances because the algorithm makes only short term decisions  about the next word without considering the output sentence as a whole. Therefore, we need to use an algorithm that would stand half-way between the exhaustive search and the greedy algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8641658",
   "metadata": {},
   "source": [
    "#### Beam Search Algorithm <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b079e3",
   "metadata": {},
   "source": [
    "    \n",
    "A better more genral algorithm is given by the Auto Regressive \\textit{Beam Search algorithm} which, given an integer parameter $B$ (in the paper $B=4$), keeps in memory the $B$-best output prior sentences, and update it at each time step. \n",
    "\n",
    "More precisely, by iteration: \n",
    "- at time step $t=1$, we keep in memory the $B$ words $\\hat Y^{<1>}=\\{\\hat y^{<1>}_1,\\dots, \\hat y^{<1>}_B\\}$ for which $p(\\hat y^{<1>}\\vert x)$ is the highest.\n",
    "- at time step $t>1$, assume that we have constructed a set\n",
    "\\begin{equation*}\n",
    "\\hat Y^{<t-1>}=\\Big\\{\\left(\\hat y^{<1>}_1,\\dots,\\hat y^{<t-1>}_1\\right),\\dots,\\left(\\hat y^{<1>}_B,\\dots,\\hat y^{<t-1>}_B\\right)\\Big\\}.\n",
    "\\end{equation*}\n",
    "We then construct updated set $\\hat Y^{<t>}$ \n",
    "\\begin{equation*}\n",
    "\\hat Y^{<t>}=\\Big\\{\\left(\\hat y^{<1,\\dots,t-1>}_1,\\hat y^{<t>}_1\\right),\\dots,\\left(\\hat y^{<1,\\dots,t-1>}_B,\\hat y^{<t>}_B\\right)\\Big\\},\n",
    "\\end{equation*}\n",
    "comprised of $B$ distinct elements, where $\\hat y^{<1,\\dots,t-1>}_i\\in \\hat Y^{<t-1>}$ and $\\hat y^{<t>}_i$ realize the $B$ highest values of\n",
    "    $p(\\hat y^{<t>}\\vert x, \\hat y^{<1,\\dots,t-1>})*p(\\hat y^{<1,\\dots,t-1>}\\vert x)$.\n",
    "\n",
    "This way, the beam search algorithm performs a greedy search on $B$ distinct beams that get updated at each time step.\n",
    "\n",
    "The computational cost then becomes $O(\\vert V\\vert^B T_y)$. Note that the case $B=1$ reduces to the greedy algorithm, while the case $B\\gg1$ corresponds to the exhaustive search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08655042",
   "metadata": {},
   "source": [
    "## Common algorithmic architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f372059",
   "metadata": {},
   "source": [
    "### Encoder/Decoder structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f741c985",
   "metadata": {},
   "source": [
    "Most of the modern approaches on machine translation rests on algorithms structured as Encoder/Decoder. \n",
    "- The task of the Encoder is to build an efficient vectorial representation $z=(z^{<1>},\\dots,z^{<T_x>})$ from the input sequence $x$.\n",
    "- The task of the Decoder is to estimate the conditional probability $p(y^{<t>}\\vert z,y^{<1>},\\dots,y^{<t-1>})$.\n",
    "\n",
    "It is known that training the Encoder and the Decoder together given language pair gives better results than training them separately. This however demands a larger dataset since we need a corpus for each language pair. This is why we use a vocabulary $V$ containing the words of the two languages at once. This also allows what allows us use the same word embedding for both the encoder and the decoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c64bcf",
   "metadata": {},
   "source": [
    "### Reccurent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b991523",
   "metadata": {},
   "source": [
    "According to the sequential nature of the problem, we traditionnaly design the Encoder and the Decoder to have a sequential architecture.\n",
    "The modern approaches use Recurrent Neural Network architectures.\n",
    "\n",
    "As the length of the input sentence increases, the model needs an increasing complexity in order to be able to carry information between distant elements of the sequence. By order of increasing complexity, one might use \n",
    "\\begin{equation*}\n",
    "\\text{Simple RNN}\n",
    "\\quad \\rightarrow\\quad\n",
    "\\text{Gated Reccurent Unit}\n",
    "\\quad \\rightarrow\\quad\n",
    "\\text{Long Short Term Memory}\n",
    "\\end{equation*}\n",
    "as reccurent units for the neural networks.\n",
    "Moreover, bi-directional RNN are usually prefered since a word in a sentence depends on both the previous and the following words.\n",
    "\n",
    "These architectures are generally used for both the Encoder and the Decoder, each of them featuring multiple layers of Bi-directionnal RNN.\n",
    "\n",
    "For the sake of completeness, let us briefly recall the definitions of the classical recurrent units. For simplicity we only present the uni-directionnal version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2b7f62",
   "metadata": {},
   "source": [
    "#### Simple RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9497ffb5",
   "metadata": {},
   "source": [
    "The simple RNN is defined by the equations:\n",
    "\\begin{gather*}\n",
    "a^{<0>}=0,\n",
    "\\\\\n",
    "a^{<t>}=\\tanh\\left(W_{a}\\left[a^{<t-1>},x^{<t>}\\right]+b_a\\right),\n",
    "    \\\\\n",
    "   \\hat y^{<t>}=\\mathrm{Softmax}\\left(W_{y} a^{<t>}+b_y\\right) \n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72f0662",
   "metadata": {},
   "source": [
    "#### Gated Recurrent Unit (GRU) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5286bca3",
   "metadata": {},
   "source": [
    "The GRU is defined by the equations:\n",
    "\\begin{align*}\n",
    "&a^{<0>}=0 &&\\text{(initial state cell)} ,\n",
    "\\\\\n",
    "&\\tilde a^{<t>}=\\tanh\\left(W_{a}\\left[\\Gamma_r*a^{<t-1>},x^{<t>}\\right]+b_a\\right),\n",
    "&&\\text{(state cell candidate update)}\n",
    "\\\\\n",
    "&\\Gamma_u=\\mathrm{sigmoid}\\left(W_u \\left[a^{<t-1>},x^{<t>}\\right]+b_u\\right)\n",
    "&&\\text{(update gate)}\n",
    "        \\\\\n",
    "& \\Gamma_r=\\mathrm{sigmoid}\\left(W_r \\left[a^{<t-1>},x^{<t>}\\right]+b_r\\right)\n",
    "   &&\\text{(relevance gate)}\n",
    " \\\\\n",
    "    & a^{<t>}=\\Gamma_u \\tilde a^{<t>}+\\left(1-\\Gamma_u\\right) a^{<t-1>},\n",
    " &&\\text{(state cell)}\n",
    "       \\\\\n",
    "   &\\hat y^{<t>}=\\mathrm{Softmax}\\left(W_{y} a^{<t>}+b_y\\right) \n",
    "   &&\\text{(output)} \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3adb99a",
   "metadata": {},
   "source": [
    "#### Long Short Term Memory unit (LSTM) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f772c7bf",
   "metadata": {},
   "source": [
    "The LSTM is defined by the equations:\n",
    "\\begin{align*}\n",
    "&a^{<0>}= c^{<0>}=0 &&\\text{(initialization)} ,\n",
    "\\\\\n",
    "&\\tilde c^{<t>}=\\tanh\\left(W_{c}\\left[\\Gamma_r a^{<t-1>},x^{<t>}\\right]+b_c\\right),\n",
    "&&\\text{(memory cell candidate update)}\n",
    "\\\\\n",
    "&\\Gamma_u=\\mathrm{sigmoid}\\left(W_u \\left[a^{<t-1>},x^{<t>}\\right]+b_u\\right)\n",
    "&&\\text{(update gate)}\n",
    "\\\\\n",
    "&\\Gamma_f=\\mathrm{sigmoid}\\left(W_f \\left[a^{<t-1>},x^{<t>}\\right]+b_f\\right)\n",
    "&&\\text{(forget gate)}\n",
    "        \\\\\n",
    "& \\Gamma_o=\\mathrm{sigmoid}\\left(W_o \\left[a^{<t-1>},x^{<t>},c^{<t-1>}\\right]+b_o\\right)\n",
    "   &&\\text{(output gate with peep-hole connection)}\n",
    " \\\\\n",
    "    & c^{<t>}=\\Gamma_u \\tilde c^{<t>}+\\Gamma_f c^{<t-1>},\n",
    " &&\\text{(memory cell)}\n",
    "     \\\\\n",
    "    & a^{<t>}=\\Gamma_o \\tanh\\left(c^{<t>}\\right),\n",
    " &&\\text{(state cell)}\n",
    "       \\\\\n",
    "   &\\hat y^{<t>}=\\mathrm{Softmax}\\left(W_{y} a^{<t>}+b_y\\right) \n",
    "   &&\\text{(output)} \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c7608",
   "metadata": {},
   "source": [
    "### additive Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fb5c65",
   "metadata": {},
   "source": [
    "As the length of sentences increases, it is necessary to include an \\textit{Attention mechanism} in the architecture. This consists in feeding each RNN layer with a \\textit{context vector} $c^{<t>}$ instead of the raw output $a^{<t>}$ of the previous RNN layer. The context $c^{<t>}$ is computed as a weighted average of the whole sequence $(a^{<t'>})_{t'}$, the weigths being a function of $(a^{<t'>})_{t'}$ and of the previous state vector $S^{<t-1>}$ of the current layer.\n",
    "More precisely, $c^{<t>}$ is computed as follows:\n",
    "\\begin{gather*}\n",
    "e^{<t,t'>}= f\\left(a^{<t'>}, S^{<t-1>}\\right),\\qquad\\qquad \\text{where $f$ is a NN with one layer},\n",
    "\\\\\n",
    "\\alpha^{<t,t'>}= \\mathrm{Softmax}_{t'}\\left(e^{<t,t'>}\\right)= \\frac{\\exp\\left(e^{<t,t'>}\\right)}{\\sum_{t''}\\exp\\left(e^{<t,t''>}\\right)}\n",
    "\\\\\n",
    " c^{<t>}= \\sum_{t'}\\alpha^{<t,t'>} a^{<t'>}.\n",
    "\\end{gather*}\n",
    "Note that the last formula is a weighted average since $\\sum_{t'}\\alpha^{<t,t'>}=1$.\n",
    "    \n",
    "The above equations define what is called an \\textbf{additive attention}. This term translates the fact that a NN with one layer is used in the computation of $e^{<t,t'>}$.\n",
    "    \n",
    "The attention mechanism allows to generate an efficient encoding of a sequence by gathering information from possibly distant elements. This property makes the Attention mechanism very powerful when dealing with long sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed79cfba",
   "metadata": {},
   "source": [
    "## Limits of the common architectures and motivations for the Transformer architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246993e0",
   "metadata": {},
   "source": [
    "The additive Attention mechanism described above has some serious computational drawbacks as the length of the sequence increases:\n",
    "\\begin{enumerate}\n",
    "\\item The computational cost of the sequence $(c^{<t>})_t$ is quadratic in the length of the sequence.\n",
    "\\item Since we use $S^{<t-1>}$ in the computation of $c^{<t>}$, the computation must be sequential and cannot be parallelized.\n",
    "\\end{enumerate}\n",
    "These observation motivate the Transformer architecture which proposes a non-sequential computation of context vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec303972",
   "metadata": {},
   "source": [
    "# the Transformer algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284b1846",
   "metadata": {},
   "source": [
    "## Overview of the architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae95e07e",
   "metadata": {},
   "source": [
    "The Transformer entirely rests on an attention mechanism instead of using reccurent units.\n",
    "This way, the Transformer gets rid of the sequential computations which allows for parallelized computations while being able to learn global dependencies between elements in a sequence.\n",
    "\n",
    "This architecture is known to give state-of-the art results in numerous application while it trains faster than most of the classical models based on RNN structures.\n",
    "\n",
    "The Transformer has an Encoder/Decoder structure. Both the Encoder and Decoder are composed of N identical layers stacked, each of them composed of a socalled Multi-head Attention layer (described below) followed by a position-wise fully connected NN, with residual connections and normalizations.\n",
    "The output of the decoder is then passed through a simple Linear+Softmax layer of size $\\vert V\\vert$.\n",
    "\n",
    "The inputs of the Encoder and Decoder are sequences of tokens encoded as one-hot vectors, which are first embedded and added to some positional encoding before being feed to the NN. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e23e639",
   "metadata": {},
   "source": [
    "<img src=\"ModalNet-21.png\" width=\"400\" height=\"200\">\n",
    " Illustration of the Transformer architecture. Source: [Attention is All you Need, Vaswani et. al. (2017)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7cf350",
   "metadata": {},
   "source": [
    "## Description of the layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f27bf9",
   "metadata": {},
   "source": [
    "### Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde36eeb",
   "metadata": {},
   "source": [
    "Let us begin with the description of the attention mechanism which is the core building block of the Transformer architecture, used in the network in several ways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f953c468",
   "metadata": {},
   "source": [
    "#### Scaled dot-product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb76e8de",
   "metadata": {},
   "source": [
    "Let us consider three sequences:\n",
    "- the values $V=(V^{<1>},\\dots,V^{<T_v>})$ of shape (batch, $T_v$, $d_v$), encoding the values of the inputs. The dimension $T_v$ stands for the length of the sequence, and the dimension $d_v$ can be seen as the embedding dimension of the inputs.\n",
    "- the queries $Q=(Q^{<1>},\\dots,Q^{<T_q>})$ of shape (batch, $T_q$, $d_k$), encoding questions on the inputs values. The dimension $d_k$ is the embedding dimension of the queries.\n",
    "- the keys $K=(K^{<1>},\\dots,K^{<T_v>})$ of shape (batch, $T_v$, $d_k$), encoding the relevance of the information contained in the values relatively to some queries, \n",
    "We define the Attention\n",
    "\\begin{equation*}\\label{attention}\n",
    "A(Q,K,V):=\\mathrm{Softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right)V\\quad\\quad\\text{of shape (batch, $T_q$, $d_v$)}\n",
    "\\end{equation*}\n",
    "\n",
    "This attention is refered to as a \\textbf{scaled dot-product} representation. It represents the information contained in the Values which are relevant to the Queries. It is computed as a weighted average of the values V, where the weights represent the compatibility of the query with the corresponding key. \n",
    "\n",
    "|Tensor Notation | Name | Shape\n",
    "| --- | --- |--- |\n",
    "|$Q$ | Queries | (batch, $T_q$, $d_k$) |\n",
    "|$K$ | Keys  | (batch, $T_v$, $d_k$) |\n",
    "| $V$ | Values | (batch, $T_v$, $d_v$) |\n",
    "|$A$ | Attention | (batch, $T_q$, $d_v$)|\n",
    "\n",
    "\n",
    "\\textbf{Remarks:}\n",
    "- the term $Q K^T$ $(=Q\\cdot K)$ of shape (batch, $T_q$, $T_v$) represents the compatibility of the query with the corresponding key.\n",
    "In other words, this term represents the relevancy of the informations contained in the values $V$ needed at the location $K$ to answer the queries $Q$.\n",
    "- The Softmax has the role of selecting the relevant information. This way, $A$ is nothing but a weighted average of the values $V$.\n",
    "- the normalization by $\\sqrt{{d_k}}$ aims at preventing the Softmax to take extreme values (for which its gradient is small which would hinger the training). Formally, $\\sqrt{{d_k}}$ normalizes to $1$ the standard deviation of $Q K^T$ seen as a random variable. This normalization allows to achieve as good performances as additive attentions as $d_k\\gg1$, while being computationally more efficient than the latter by leveraging optimized matrix-multiplication algorithms.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0b3ef5",
   "metadata": {},
   "source": [
    "#### Multi-head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67252d24",
   "metadata": {},
   "source": [
    "Let us consider a collection of $H\\ge 1$ triplets $(Q_h,K_h,V_h)_{1\\le h\\le H}$ as described above. We can think of each triplet $Q_h,K_h,V_h$ as being derived after applying (learned) projectors $(W_i^Q,W_i^K,W_i^V)$ from bigger tensors $(Q,K,V)$ of dimension $d_{model}$ as follows: for all $1\\le h\\le H$,\n",
    "\\begin{gather*}\n",
    "Q_h:= Q W_i^Q,\n",
    "\\\\\n",
    "K_h:= K W_i^K,\n",
    "\\\\\n",
    "V_h:= V V_i^Q.\n",
    "\\end{gather*}\n",
    "\n",
    "We then set $A_h:= A(Q_h,K_h,V_h)$ called an \\textbf{Attention head}. The concatenation of the heads $A=\\mathrm{Concat}(A_1,\\dots,A_H)W_0$ is then called a \\textbf{Multi-head Attention}, where $W^O$ is a projection from $H\\times d_v$ onto $d_{model}$.\n",
    "\n",
    "This way of projecting, concatening, and projecting again allows to break down the $d_{model}$-dimensional computations into $H$ \\textbf{parallelized} ($d_k$, $d_v$)-dimensional computations.\n",
    "\n",
    "The other advantage of this method is that we can take $Q=K=V$. In this case, we call the mechanism a \\textbf{self attention} mechanism. This is used in the Transformer at the bottom of each layer of the Encoder and the Decoder (see the section \"Use of Multi-head attention in the Transformer\" below) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40283ab",
   "metadata": {},
   "source": [
    "| Tensor Notation | Name | Shape |Computation formulae from other tensors\n",
    "| --- | --- |--- | --- |\n",
    "|$Q$ | Queries | (batch, $T_q$, $d_{model}$) | |\n",
    "|$K$| Keys | (batch, $T_v$, $d_{model}$) | |\n",
    "|$V$| Values | (batch, $T_v$, $d_{model}$) | |\n",
    "|$W_h^Q$ | Queries projector$_h$  | ($d_{model}$,$d_k$)| |\n",
    "|$W_h^K$| Keys projector$_h$ | ($d_{model}$,$d_k$)| |\n",
    "|$W_h^V$ | Values projector$_h$ | ($d_{model}$,$d_v$)| |\n",
    "|$W^O$ | Multi-head projector | ($H\\times d_v$, $d_{model}$)| |\n",
    "|$Q_h$| projected Queries | (batch, $T_q$, $d_k$) | $QW_h^Q$|\n",
    "|$K_h$| projected Keys | (batch, $T_v$, $d_k$) | $KW_h^K $|\n",
    "|$V_h$| projected Values  | (batch, $T_v$, $d_v$) | $VW_h^V$|\n",
    "|$A_h$| Attention-head$_h$  | (batch, $T_q$, $d_v$)| $\\mathrm{Softmax}\\left(\\frac{Q_h K_h^T}{\\sqrt{d_k}}\\right)V_h$|\n",
    "|$A$| Multi-head Attention  | (batch, $T_q$, $d_{model}$)|$\\mathrm{Concat}(A_1,\\dots,A_H)W_0$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046dde15",
   "metadata": {},
   "source": [
    "#### Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d1d137",
   "metadata": {},
   "source": [
    "In the paper, dimensions are chosen as follows:\n",
    "\n",
    "|Notation | Numerical value | Description\n",
    "| --- | --- |--- |\n",
    "|$d_{model}$ | 512 | Multi-head Attention embedding|\n",
    "|$d_k$ | 64  | queries/keys embedding |\n",
    "| $d_v$ | 64 | values embedding |\n",
    "|$H$ | 8 | number of heads|\n",
    "\n",
    "Note that the numerical values are such that $d_k=d_v=\\frac{d_{model}}{H}$. This way, the Multi-head projector $W^O$ is a square matrix and can be chosen as being the identity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce384121",
   "metadata": {},
   "source": [
    "#### Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7c5e63",
   "metadata": {},
   "source": [
    "Self attention refers to the case where that the Queries, Keys, and Values tensors are taken equal. In this case, we have\n",
    "\\begin{gather*}\n",
    "Q=K=V=:I,\n",
    "\\\\\n",
    "A_h=\\mathrm{Softmax}\\left(\\frac{IW^Q_h (W^K_h)^T I^T}{\\sqrt{d_k}}\\right)IW_h^V.\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e345e14f",
   "metadata": {},
   "source": [
    "#### Comparison with the classical previous models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f753a0e1",
   "metadata": {},
   "source": [
    "The main benefit of the attention mechanism is he ability of linking two distant elements of the input sequence. Indeed, the attention mechanism uses a fixed number of operation to link arbitrarily distant words of the input sequence, whereas other classical architecture like RNN and CNN need to increase the number of operation as the distance between the two elements increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cd935a",
   "metadata": {},
   "source": [
    "The dot-product Attention mechanism of the Transformer is computed globally whereas the classical additive attention mechanism is computed sequentially (because it uses the state cell of the current layer at previous time). This allows to leverage parallelization and thus to reduce the computational complexity.\n",
    "Another advantage of the dot-product attention over the additive dot product is that the former is more computationally efficient since it can use optimized matrix-multiplication algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa8f8c5",
   "metadata": {},
   "source": [
    "To give an intuitive idea of how additive and dot-product attention mechanism relate, we propose the following formal correspondances (which are not intended to be exact):\n",
    "\n",
    "|Tensor in dot-product Attention mechanism | formal equivalent in additive Attention mechanism\n",
    "| --- | --- |\n",
    "|Values $V$ | Input tensor $a=(a^{<1>},\\dots,a^{T})$|\n",
    "|Queries $Q$ | State cell of the current layer at previous time $S^{<t-1>}$ |\n",
    "|Dot-product $Q K^T$ | Matrix of the $e^{<t,t'>}$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f4059f",
   "metadata": {},
   "source": [
    "Finally, the authors claim that the Transformer architecture would yield more interpretable models, by exhibiting behavior related to the syntactic and semantic structure of the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777cb06c",
   "metadata": {},
   "source": [
    "#### Use of Multi-head attention in the Transformer \\label{sec:UtilizationAttention}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242f0e66",
   "metadata": {},
   "source": [
    "The Multi-head attention is used in three different ways:\n",
    "- \\textbf{A Self-attention} unit at the bottom of each layer of the Encoder. We take $Q=K=V=I$ where $I$ (the input of the layer) is the output of the previous layer.\n",
    "- \\textbf{A masked Self-attention} unit at the bottom of each layer of the Decoder, which is similar to the one in the Encoder that we have just described. Here, the mask is used to hide the elements of the input sequence of the decoder which are subsequent to some position $t$. This is usefull for technical reason when using auto regressive generation, in order to be able to train the model to predict the next word at step $t+1$ without accessing the words after $t$ in the input sequence\n",
    "- \\textbf{An Encoder-Decoder Attention} unit in the Decoder layers, where the Queries come from the previous layer, while the Keys and Values are the output of the Encoder (and thus $K=V$). Note that the Keys and Values contain information on the whole input sequence $x$. Note also that the Encoder and Decoder are linked only through these layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f760b6b",
   "metadata": {},
   "source": [
    "### (Point-wise) Feed forward NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579bb172",
   "metadata": {},
   "source": [
    "This layer is composed of two stacked 1D-Convolutional Neural Network with kernel-size of $1$. This way, each time step is processed separately and identically. We apply a ReLu nonlinearity between the two layers.\n",
    "\\begin{equation*}\n",
    "\\mathrm{FFN}(x)= \\mathrm{ReLu}(x W1+b1)W2+b2,\n",
    "\\end{equation*}\n",
    "with input and output dimension being $d_{model}$ and the inner dimension being $d_{ff}$ (which equals 2048 in the paper)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164023cc",
   "metadata": {},
   "source": [
    "\n",
    "|Name of the tensor| Shape of the tensor|\n",
    "| --- | --- |\n",
    "| $x$ | (batch, T, $d_{model}$)|\n",
    "|$W_1$ | ($d_{model}$, $d_{ff}$)|\n",
    "|$b_1$ | (1, $d_{ff}$)|\n",
    "|$W_2$ | ($d_{ff}$, $d_{model}$)|\n",
    "|$b_1$ | (1, $d_{model}$)|\n",
    "| $\\mathrm{FFN}(x)$| (batch, T, $d_{model}$)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbadd722",
   "metadata": {},
   "source": [
    "### Embeddings and Softmax layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76666f19",
   "metadata": {},
   "source": [
    "As it is traditionally the case, the input sequences are tokens encoded as one-hot tensors of shape (batch, sequence_length, $\\vert V\\vert$). The Embedding layer consists of a learned matrix $\\sqrt{d_k}U$ of shape $(\\vert V\\vert, d_{model})$. The coefficient $\\sqrt{d_k}$ is used for renormalization.\n",
    "\n",
    "In the Transformer architecture, the two Embedding layers (at the bottom of the Encoder and Decoder respectively) share the same weight matrix $\\sqrt{d_k}U$. This way, the Embedding yields vector representations of the words regardless of the language they belong to.\n",
    "\n",
    "The output of the Decoder passes through a Linear+Softmax layer which gives the conditional probabilities of the next word. The weight matrix of this linear layer is chosen as to be the transposed of the matrix $U$ used in the Embedding layer of the Decoder; in other word, the output embedding share the same weight as the input embedding (except for the renormalization factor $\\sqrt{d_k}$). This is motivated by the fact that we want to inverse the embedding to recover the one-hot representation of the token. Imposing these share weights is known to improve the performances of the model [Using the output embedding to improve language models, Press et. al. 2017]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe1504a",
   "metadata": {},
   "source": [
    "### Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e613da",
   "metadata": {},
   "source": [
    "All the layers in the Transformer do not take into account the position of the element in the sequence. However, since the order of the words in a sentence carries significant importance, we have to encode the absolute and/or relative position of each element of the input sequence. This motivates the use of target encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22150e60",
   "metadata": {},
   "source": [
    "The Transfomer uses a fixed (and not learned) $d_{model}-dimensional$ positional encoding through sine and cosine functions. We set\n",
    "\\begin{gather*}\n",
    "PE_{pos,2i}=\\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)\n",
    "\\\\\n",
    "PE_{pos,2i}=\\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)\n",
    "\\end{gather*}\n",
    "This choice is motivated by the fact that, for a fixed offset $k$, $PE_{pos+k}$ can be written as a linear function of $PE_{pos}$ (through the addition formulae of trigonometric functions), which would make the model able to acount for relative positions of the elements in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a024715",
   "metadata": {},
   "source": [
    "### Normalization layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dccaed",
   "metadata": {},
   "source": [
    "The Normalization Layer (introduced in [Layer Normalization, Lei Ba (2016)]) takes as input a (batch of) sequence $a=(a^{<1>},\\dots,a^{<T>})$ of shape (batch, $T$, $d_{model}$) and computes the empirical mean and variance element-wise as follows\n",
    "\\begin{gather*}\n",
    "\\mu^{<t>}=\\frac{1}{d_{model}}\\sum_{i=1}^{d_{model}} a^{<t>}_i\n",
    "    \\\\\n",
    "\\sigma^{<t>}=\\sqrt{\\frac{1}{d_{model}}\\sum_{i=1}^{d_{model}} \\left(a^{<t>}_i-\\mu^{<t>}\\right)^2}\n",
    "\\end{gather*}\n",
    "and returns\n",
    "    \\begin{equation*}\n",
    "    \\mathrm{LayerNorm}(a)= \\beta\\frac{a-\\mu}{\\sigma+\\eta}+\\alpha,\n",
    "    \\end{equation*}\n",
    "where $\\alpha$, $\\beta$ are learned parameters of shape (batch, $T$, 1), and $\\eta>0$ is a small fixed parameter to avoid division by zero (we can take $\\eta=10^{-6}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a2792b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eca18b",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea858a51",
   "metadata": {},
   "source": [
    "As is commonly done for any classification problem, we consider the usual cross-entropy loss, defined as follows: given a label and prediction pair $y,\\hat y\\in\\mathbb{R}^{\\vert V\\vert}$,\n",
    "\\begin{equation*}\n",
    "\\mathcal{L}(y,\\hat y)= \\frac{1}{m}\\sum_{k=1}^m \\sum_{t=1}^{T_y}y^{<t+1>}_{k}\\log\\hat y^{<t>}_{k},\n",
    "\\end{equation*}\n",
    "where $m$ is the batch size and the subscript $(k)$ indexes the different datapoints in the batch. Note the offset in the sequence index $<t>$ between $y$ and $\\hat y$ which translates that we train the model on predicting the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a10c17",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e8130d",
   "metadata": {},
   "source": [
    "#### Adam Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c233b009",
   "metadata": {},
   "source": [
    "The optimization is done through Adam stochastic gradient descent. Let $\\alpha,\\beta_1,\\beta_2,\\epsilon>0$ be hyperparameters, consider $W(t)$ a learned parameter initialized at some $W(0)$ and set $dW(t)=\\nabla_{W}\\mathcal{L}(t)$. The adam optimizer is defined by the equations:\n",
    "\\begin{gather*}\n",
    "V(t+1)=\\beta_1 V(t)+ (1-\\beta_1)dW(t),\n",
    "\\\\\n",
    "S(t+1)=\\beta_2 S(t)+ (1-\\beta_2)dW^2(t)\\quad \\text{  (the square is taken element-wise)}, \n",
    "\\\\\n",
    "\\hat V = \\frac{V(t+1)}{1-\\beta_1},\n",
    "\\\\\n",
    "\\hat S = \\frac{S(t+1)}{1-\\beta_2},\n",
    "\\\\\n",
    "W(t+1)= W(t) - \\alpha \\frac{\\hat V}{\\sqrt{\\hat S}+\\epsilon}\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242f88f1",
   "metadata": {},
   "source": [
    "#### Learning rate scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab8e84",
   "metadata": {},
   "source": [
    "We choose the learning rate $\\alpha$ to be time dependent. The learning rate is initialized at $0$. First, the learning increases linerarily during a warm up phase and then decreases as an inverse square root function. It is given by the following formula:\n",
    "\\begin{equation*}\n",
    "\\alpha(t)= \\frac{1}{\\sqrt{d_{model}}}\\min\\left(\\frac{1}{\\sqrt{t}}, \\text{warmup_steps}^{-3/2}\\cdot t \\right)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f693a5",
   "metadata": {},
   "source": [
    "#### Choice for the learning hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1117f1",
   "metadata": {},
   "source": [
    "We choose the following values of parameters\n",
    "\n",
    "|Notation of the parameter| Chosen numerical value |\n",
    "| --- | --- |\n",
    "| warmup_steps | $4000$|\n",
    "|$\\beta_1$ | $0.9$|\n",
    "|$\\beta_2$ | $0.98$|\n",
    "|$\\epsilon$ | $10^{-9}$|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b0f1d6",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5702d45",
   "metadata": {},
   "source": [
    "#### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3a5a43",
   "metadata": {},
   "source": [
    "A residual Dropout with 10% dropout is applied after the positional encoding and before all the residual connections in the layers of the Encoder and Decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7119f86f",
   "metadata": {},
   "source": [
    "#### Label smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e23a3a",
   "metadata": {},
   "source": [
    "Given a parameter $\\delta$ (here we choose $\\delta=0.1$), smoothing consists in replacing, in the one-hot representation of the labels $y$, the vaue \"$1$\" by $1-\\delta$, and the values \"0\" by $\\frac{\\delta}{\\vert V\\vert -1}$. This procedure adds noise and train the model to be more uncertain; yet it is known to help training and to achieve better results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648d3900",
   "metadata": {},
   "source": [
    "### Dataset and batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828330cf",
   "metadata": {},
   "source": [
    "The algorithm has been trained on two different dataset:\n",
    "- English/German datastet (WMT 2014) containing $\\sim$4.5 million sentence pairs, with $\\vert V\\vert \\sim37000$,\n",
    "- English/French dataset (WMT 2014) containing $\\sim$36 million sentence pairs, with $\\vert V\\vert \\sim32000$.\n",
    "\n",
    "The dataset is split into mini-batch containing 25000 token pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa9bd1f",
   "metadata": {},
   "source": [
    "### Hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e23e0c4",
   "metadata": {},
   "source": [
    "- The base model is trained on 8 NVIDIA P100 GPUs for 12h.\n",
    "- A bigger model is also trained for 3.5 days.\n",
    "\n",
    "These training times are significantly lower than the available standard at time of publication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3b24a7",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5076600e",
   "metadata": {},
   "source": [
    "The performance of the algorithm is evualuated on the prediction of whole sentences using the Auto regression method and Beam search algorithm presented in the introduction.\n",
    "The Beam search algorithm is used with parameter $B=4$, and length normalization with parameter $\\alpha=0.6$.\n",
    "\n",
    "The final output prediction is actual deduced as the average of the 5 or 10 last saved checkpoint of the model (during training)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c0f776",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e797dbe0",
   "metadata": {},
   "source": [
    "Let us present the possible metrics available to measure the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bd3fee",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8388a9e3",
   "metadata": {},
   "source": [
    "A simple metric that can be used in this context is the simple Accuracy score, which counts good predictions as 1 and wrong predictions as 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9b81f5",
   "metadata": {},
   "source": [
    "#### BLEU score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c00921",
   "metadata": {},
   "source": [
    "In the context of translation, the accuracy is often replaced by other metrics to account for the fact that different translations of the same input can be equally as good, i.e., we dispose of several labels for the same input. We can use the BLEU score [BLEU: a Method for Automatic Evaluation of Machine Translation, Papineni et. al. (2002)]. Let us we fix an integer $n\\ge 1$, a prediction (called \"candidates\" in this context) and a set of good labels (called \"references\" in this context). For each n-gram C present in the candidate, let Count(C) be the number of times C appears in the prediction, and let $MaxCount_{ref}(C)$ be the maximum number of times C appears in any of the references. The BLEU score is then given by\n",
    "\\begin{equation*}\n",
    "p_n= \\frac{\\sum\\limits_{C\\in \\text{n-gram} } \\min\\left(Count(C),MaxCount_{ref}(C)\\right)}\n",
    "{\\sum\\limits_{C\\in \\text{n-gram} } Count(C)}\n",
    "\\end{equation*}\n",
    "A typical choice for $n$ is $n=4$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0af502",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7263c6b2",
   "metadata": {},
   "source": [
    "The model achieves the following BLEU scores:\n",
    "\n",
    "|Model| English/German | English/French |\n",
    "| --- | --- |--- |\n",
    "| base model | 27.3| 38.1|\n",
    "|big model | 28.4 | 41.8 |\n",
    "\n",
    "The big model (and the base model for the English/German dataset) outperforms all the best other available models at the time of publication, while being trained during a much shorter period."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "466px",
    "left": "110px",
    "top": "139px",
    "width": "252px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
